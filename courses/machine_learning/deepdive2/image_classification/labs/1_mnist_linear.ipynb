{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image Classification with TensorFlow\n",
    "\n",
    "This notebook demonstrates how to implement a simple linear image model on [MNIST](http://yann.lecun.com/exdb/mnist/) using the [tf.keras API](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras). It builds the foundation for this <a href=\"https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/image_classification/labs/2_mnist_models.ipynb\">companion notebook</a>, which explores tackling the same problem with other types of models such as DNN and CNN.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Know how to read and display image data\n",
    "2. Know how to find incorrect predictions to analyze the model\n",
    "3. Visually see how computers see images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses TF2.x.\n",
    "Please check your tensorflow version using the cell below. If it is 2.x, navigate to **Exploring the data** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Flatten, Softmax\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is not 2.x, please run the pip command below and **Restart** the kernel. Also, **run** the above command again to set all the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==2.5.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip freeze | grep 'tensorflow==2\\|tensorflow-gpu==2' || \\\n",
    "    python3 -m pip install tensorflow==2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "\n",
    "The MNIST dataset is already included in tensorflow through the keras datasets module. Let's load it and get a sense of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image height x width is 28 x 28\n",
      "There are 10 classes\n"
     ]
    }
   ],
   "source": [
    "HEIGHT, WIDTH = x_train[0].shape\n",
    "NCLASSES = tf.size(tf.unique(y_train).y)\n",
    "print(\"Image height x width is\", HEIGHT, \"x\", WIDTH)\n",
    "tf.print(\"There are\", NCLASSES, \"classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is 28 x 28 pixels and represents a digit from 0 to 9. These images are black and white, so each pixel is a value from 0 (white) to 255 (black). Raw numbers can be hard to interpret sometimes, so we can plot the values to see the handwritten digit as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label for image number 12 is 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "IMGNO = 12\n",
    "# Uncomment to see raw numerical values.\n",
    "# print(x_test[IMGNO])\n",
    "plt.imshow(x_test[IMGNO].reshape(HEIGHT, WIDTH));\n",
    "print(\"The label for image number\", IMGNO, \"is\", y_test[IMGNO])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "Let's start with a very simple linear classifier. This was the first method to be tried on MNIST in 1998, and scored an 88% accuracy. Quite ground breaking at the time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build our linear classifier using the [tf.keras API](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras), so we don't have to define or initialize our weights and biases. This happens automatically for us in the background. We can also add a softmax layer to transform the logits into probabilities. Finally, we can compile the model using categorical cross entropy in order to strongly penalize high probability predictions that were incorrect.\n",
    "\n",
    "When building more complex models such as DNNs and CNNs our code will be more readable by using the [tf.keras API](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras). Let's get one working so we can test it and use it as a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model():\n",
    "    # TODO: Build a sequential model and compile it.\n",
    "    model = Sequential([\n",
    "        Flatten(),\n",
    "        Dense(NCLASSES),\n",
    "        Softmax()\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Input Functions\n",
    "\n",
    "As usual, we need to specify input functions for training and evaluating. We'll scale each pixel value so it's a decimal value between 0 and 1 as a way of normalizing the data.\n",
    "\n",
    "**TODO 1**: Define the scale function below and build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 5000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def load_dataset(training=True):\n",
    "    \"\"\"Loads MNIST dataset into a tf.data.Dataset\"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = mnist\n",
    "    x = x_train if training else x_test\n",
    "    y = y_train if training else y_test\n",
    "    # TODO: a) one-hot encode labels, apply `scale` function, and create dataset.\n",
    "    y = tf.keras.utils.to_categorical(y, NCLASSES)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.map(scale).batch(BATCH_SIZE)\n",
    "    # One-hot encode the classes\n",
    "    if training:\n",
    "         dataset = dataset.shuffle(BUFFER_SIZE).repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for training passed!\n",
      "Test for eval passed!\n"
     ]
    }
   ],
   "source": [
    "def create_shape_test(training):\n",
    "    dataset = load_dataset(training=training)\n",
    "    data_iter = dataset.__iter__()\n",
    "    (images, labels) = data_iter.get_next()\n",
    "    expected_image_shape = (BATCH_SIZE, HEIGHT, WIDTH)\n",
    "    expected_label_ndim = 2\n",
    "    assert(images.shape == expected_image_shape)\n",
    "    assert(labels.numpy().ndim == expected_label_ndim)\n",
    "    test_name = 'training' if training else 'eval'\n",
    "    print(\"Test for\", test_name, \"passed!\")\n",
    "\n",
    "\n",
    "create_shape_test(True)\n",
    "create_shape_test(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model! The original MNIST linear classifier had an error rate of 12%. Let's use that to sanity check that our model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 - 4s - loss: 1.3546 - accuracy: 0.6482 - val_loss: 0.8102 - val_accuracy: 0.8271\n",
      "\n",
      "Epoch 00001: saving model to mnist_linear/\n",
      "Epoch 2/50\n",
      "100/100 - 0s - loss: 0.6914 - accuracy: 0.8390 - val_loss: 0.5664 - val_accuracy: 0.8690\n",
      "\n",
      "Epoch 00002: saving model to mnist_linear/\n",
      "Epoch 3/50\n",
      "100/100 - 0s - loss: 0.5495 - accuracy: 0.8661 - val_loss: 0.4765 - val_accuracy: 0.8852\n",
      "\n",
      "Epoch 00003: saving model to mnist_linear/\n",
      "Epoch 4/50\n",
      "100/100 - 0s - loss: 0.4681 - accuracy: 0.8795 - val_loss: 0.4247 - val_accuracy: 0.8917\n",
      "\n",
      "Epoch 00004: saving model to mnist_linear/\n",
      "Epoch 5/50\n",
      "100/100 - 0s - loss: 0.4267 - accuracy: 0.8873 - val_loss: 0.3941 - val_accuracy: 0.8988\n",
      "\n",
      "Epoch 00005: saving model to mnist_linear/\n",
      "Epoch 6/50\n",
      "100/100 - 0s - loss: 0.3944 - accuracy: 0.8940 - val_loss: 0.3707 - val_accuracy: 0.9036\n",
      "\n",
      "Epoch 00006: saving model to mnist_linear/\n",
      "Epoch 7/50\n",
      "100/100 - 0s - loss: 0.3797 - accuracy: 0.8970 - val_loss: 0.3546 - val_accuracy: 0.9072\n",
      "\n",
      "Epoch 00007: saving model to mnist_linear/\n",
      "Epoch 8/50\n",
      "100/100 - 0s - loss: 0.3724 - accuracy: 0.9004 - val_loss: 0.3438 - val_accuracy: 0.9088\n",
      "\n",
      "Epoch 00008: saving model to mnist_linear/\n",
      "Epoch 9/50\n",
      "100/100 - 0s - loss: 0.3602 - accuracy: 0.9014 - val_loss: 0.3345 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00009: saving model to mnist_linear/\n",
      "Epoch 10/50\n",
      "100/100 - 0s - loss: 0.3285 - accuracy: 0.9090 - val_loss: 0.3260 - val_accuracy: 0.9108\n",
      "\n",
      "Epoch 00010: saving model to mnist_linear/\n",
      "Epoch 11/50\n",
      "100/100 - 0s - loss: 0.3443 - accuracy: 0.9074 - val_loss: 0.3187 - val_accuracy: 0.9120\n",
      "\n",
      "Epoch 00011: saving model to mnist_linear/\n",
      "Epoch 12/50\n",
      "100/100 - 0s - loss: 0.3332 - accuracy: 0.9078 - val_loss: 0.3121 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00012: saving model to mnist_linear/\n",
      "Epoch 13/50\n",
      "100/100 - 0s - loss: 0.3105 - accuracy: 0.9138 - val_loss: 0.3093 - val_accuracy: 0.9145\n",
      "\n",
      "Epoch 00013: saving model to mnist_linear/\n",
      "Epoch 14/50\n",
      "100/100 - 0s - loss: 0.3285 - accuracy: 0.9071 - val_loss: 0.3039 - val_accuracy: 0.9150\n",
      "\n",
      "Epoch 00014: saving model to mnist_linear/\n",
      "Epoch 15/50\n",
      "100/100 - 0s - loss: 0.3131 - accuracy: 0.9156 - val_loss: 0.3036 - val_accuracy: 0.9168\n",
      "\n",
      "Epoch 00015: saving model to mnist_linear/\n",
      "Epoch 16/50\n",
      "100/100 - 0s - loss: 0.3184 - accuracy: 0.9119 - val_loss: 0.2969 - val_accuracy: 0.9180\n",
      "\n",
      "Epoch 00016: saving model to mnist_linear/\n",
      "Epoch 17/50\n",
      "100/100 - 0s - loss: 0.3182 - accuracy: 0.9136 - val_loss: 0.2973 - val_accuracy: 0.9163\n",
      "\n",
      "Epoch 00017: saving model to mnist_linear/\n",
      "Epoch 18/50\n",
      "100/100 - 0s - loss: 0.2988 - accuracy: 0.9152 - val_loss: 0.2949 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00018: saving model to mnist_linear/\n",
      "Epoch 19/50\n",
      "100/100 - 0s - loss: 0.2870 - accuracy: 0.9206 - val_loss: 0.2900 - val_accuracy: 0.9180\n",
      "\n",
      "Epoch 00019: saving model to mnist_linear/\n",
      "Epoch 20/50\n",
      "100/100 - 0s - loss: 0.2900 - accuracy: 0.9199 - val_loss: 0.2897 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00020: saving model to mnist_linear/\n",
      "Epoch 21/50\n",
      "100/100 - 0s - loss: 0.2915 - accuracy: 0.9217 - val_loss: 0.2940 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00021: saving model to mnist_linear/\n",
      "Epoch 22/50\n",
      "100/100 - 0s - loss: 0.3057 - accuracy: 0.9137 - val_loss: 0.2877 - val_accuracy: 0.9190\n",
      "\n",
      "Epoch 00022: saving model to mnist_linear/\n",
      "Epoch 23/50\n",
      "100/100 - 0s - loss: 0.2911 - accuracy: 0.9191 - val_loss: 0.2839 - val_accuracy: 0.9202\n",
      "\n",
      "Epoch 00023: saving model to mnist_linear/\n",
      "Epoch 24/50\n",
      "100/100 - 0s - loss: 0.3111 - accuracy: 0.9098 - val_loss: 0.2807 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00024: saving model to mnist_linear/\n",
      "Epoch 25/50\n",
      "100/100 - 0s - loss: 0.2878 - accuracy: 0.9210 - val_loss: 0.2829 - val_accuracy: 0.9204\n",
      "\n",
      "Epoch 00025: saving model to mnist_linear/\n",
      "Epoch 26/50\n",
      "100/100 - 0s - loss: 0.2896 - accuracy: 0.9222 - val_loss: 0.2795 - val_accuracy: 0.9226\n",
      "\n",
      "Epoch 00026: saving model to mnist_linear/\n",
      "Epoch 27/50\n",
      "100/100 - 0s - loss: 0.2910 - accuracy: 0.9187 - val_loss: 0.2838 - val_accuracy: 0.9204\n",
      "\n",
      "Epoch 00027: saving model to mnist_linear/\n",
      "Epoch 28/50\n",
      "100/100 - 0s - loss: 0.2696 - accuracy: 0.9250 - val_loss: 0.2799 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00028: saving model to mnist_linear/\n",
      "Epoch 29/50\n",
      "100/100 - 0s - loss: 0.2982 - accuracy: 0.9147 - val_loss: 0.2780 - val_accuracy: 0.9211\n",
      "\n",
      "Epoch 00029: saving model to mnist_linear/\n",
      "Epoch 30/50\n",
      "100/100 - 0s - loss: 0.2744 - accuracy: 0.9216 - val_loss: 0.2780 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00030: saving model to mnist_linear/\n",
      "Epoch 31/50\n",
      "100/100 - 0s - loss: 0.2868 - accuracy: 0.9209 - val_loss: 0.2754 - val_accuracy: 0.9220\n",
      "\n",
      "Epoch 00031: saving model to mnist_linear/\n",
      "Epoch 32/50\n",
      "100/100 - 0s - loss: 0.2897 - accuracy: 0.9172 - val_loss: 0.2738 - val_accuracy: 0.9237\n",
      "\n",
      "Epoch 00032: saving model to mnist_linear/\n",
      "Epoch 33/50\n",
      "100/100 - 0s - loss: 0.2703 - accuracy: 0.9268 - val_loss: 0.2736 - val_accuracy: 0.9243\n",
      "\n",
      "Epoch 00033: saving model to mnist_linear/\n",
      "Epoch 34/50\n",
      "100/100 - 0s - loss: 0.2791 - accuracy: 0.9243 - val_loss: 0.2755 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00034: saving model to mnist_linear/\n",
      "Epoch 35/50\n",
      "100/100 - 0s - loss: 0.2802 - accuracy: 0.9204 - val_loss: 0.2727 - val_accuracy: 0.9238\n",
      "\n",
      "Epoch 00035: saving model to mnist_linear/\n",
      "Epoch 36/50\n",
      "100/100 - 0s - loss: 0.2618 - accuracy: 0.9267 - val_loss: 0.2744 - val_accuracy: 0.9238\n",
      "\n",
      "Epoch 00036: saving model to mnist_linear/\n",
      "Epoch 37/50\n",
      "100/100 - 0s - loss: 0.2711 - accuracy: 0.9257 - val_loss: 0.2716 - val_accuracy: 0.9246\n",
      "\n",
      "Epoch 00037: saving model to mnist_linear/\n",
      "Epoch 38/50\n",
      "100/100 - 0s - loss: 0.2723 - accuracy: 0.9245 - val_loss: 0.2716 - val_accuracy: 0.9234\n",
      "\n",
      "Epoch 00038: saving model to mnist_linear/\n",
      "Epoch 39/50\n",
      "100/100 - 0s - loss: 0.2724 - accuracy: 0.9229 - val_loss: 0.2705 - val_accuracy: 0.9235\n",
      "\n",
      "Epoch 00039: saving model to mnist_linear/\n",
      "Epoch 40/50\n",
      "100/100 - 0s - loss: 0.2655 - accuracy: 0.9251 - val_loss: 0.2729 - val_accuracy: 0.9232\n",
      "\n",
      "Epoch 00040: saving model to mnist_linear/\n",
      "Epoch 41/50\n",
      "100/100 - 0s - loss: 0.2826 - accuracy: 0.9212 - val_loss: 0.2724 - val_accuracy: 0.9227\n",
      "\n",
      "Epoch 00041: saving model to mnist_linear/\n",
      "Epoch 42/50\n",
      "100/100 - 0s - loss: 0.2676 - accuracy: 0.9233 - val_loss: 0.2715 - val_accuracy: 0.9253\n",
      "\n",
      "Epoch 00042: saving model to mnist_linear/\n",
      "Epoch 43/50\n",
      "100/100 - 0s - loss: 0.2637 - accuracy: 0.9290 - val_loss: 0.2692 - val_accuracy: 0.9254\n",
      "\n",
      "Epoch 00043: saving model to mnist_linear/\n",
      "Epoch 44/50\n",
      "100/100 - 0s - loss: 0.2626 - accuracy: 0.9261 - val_loss: 0.2681 - val_accuracy: 0.9258\n",
      "\n",
      "Epoch 00044: saving model to mnist_linear/\n",
      "Epoch 45/50\n",
      "100/100 - 0s - loss: 0.2864 - accuracy: 0.9217 - val_loss: 0.2726 - val_accuracy: 0.9246\n",
      "\n",
      "Epoch 00045: saving model to mnist_linear/\n",
      "Epoch 46/50\n",
      "100/100 - 0s - loss: 0.2684 - accuracy: 0.9261 - val_loss: 0.2702 - val_accuracy: 0.9257\n",
      "\n",
      "Epoch 00046: saving model to mnist_linear/\n",
      "Epoch 47/50\n",
      "100/100 - 0s - loss: 0.2633 - accuracy: 0.9239 - val_loss: 0.2680 - val_accuracy: 0.9248\n",
      "\n",
      "Epoch 00047: saving model to mnist_linear/\n",
      "Epoch 48/50\n",
      "100/100 - 0s - loss: 0.2641 - accuracy: 0.9261 - val_loss: 0.2692 - val_accuracy: 0.9244\n",
      "\n",
      "Epoch 00048: saving model to mnist_linear/\n",
      "Epoch 49/50\n",
      "100/100 - 0s - loss: 0.2836 - accuracy: 0.9254 - val_loss: 0.2681 - val_accuracy: 0.9273\n",
      "\n",
      "Epoch 00049: saving model to mnist_linear/\n",
      "Epoch 50/50\n",
      "100/100 - 0s - loss: 0.2614 - accuracy: 0.9255 - val_loss: 0.2675 - val_accuracy: 0.9257\n",
      "\n",
      "Epoch 00050: saving model to mnist_linear/\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "STEPS_PER_EPOCH = 100\n",
    "\n",
    "model = linear_model()\n",
    "train_data = load_dataset()\n",
    "validation_data = load_dataset(training=False)\n",
    "\n",
    "OUTDIR = \"mnist_linear/\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    OUTDIR, save_weights_only=True, verbose=1)\n",
    "tensorboard_callback = TensorBoard(log_dir=OUTDIR)\n",
    "\n",
    "history = model.fit(\n",
    "    # TODO: specify training/eval data, # epochs, steps per epoch.\n",
    "    train_data, \n",
    "    validation_data=validation_data,\n",
    "    epochs=NUM_EPOCHS, \n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_callback, tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test to beat benchmark accuracy passed!\n",
      "Test model accuracy is improving passed!\n",
      "Test loss is decreasing passed!\n"
     ]
    }
   ],
   "source": [
    "BENCHMARK_ERROR = .12\n",
    "BENCHMARK_ACCURACY = 1 - BENCHMARK_ERROR\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "    \n",
    "assert(accuracy[-1] > BENCHMARK_ACCURACY)\n",
    "assert(val_accuracy[-1] > BENCHMARK_ACCURACY)\n",
    "print(\"Test to beat benchmark accuracy passed!\")\n",
    "        \n",
    "assert(accuracy[0] < accuracy[1])\n",
    "assert(accuracy[1] < accuracy[-1])\n",
    "assert(val_accuracy[0] < val_accuracy[1])\n",
    "assert(val_accuracy[1] < val_accuracy[-1])\n",
    "print(\"Test model accuracy is improving passed!\")\n",
    "    \n",
    "assert(loss[0] > loss[1])\n",
    "assert(loss[1] > loss[-1])\n",
    "assert(val_loss[0] > val_loss[1])\n",
    "assert(val_loss[1] > val_loss[-1])\n",
    "print(\"Test loss is decreasing passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were you able to get an accuracy of over 90%? Not bad for a linear estimator! Let's make some predictions and see if we can find where the model has trouble. Change the range of values below to find incorrect predictions, and plot the corresponding images. What would you have guessed for these images?\n",
    "\n",
    "**TODO 2**: Change the range below to find an incorrect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image number: 8\n",
      "the prediction was 6\n",
      "the actual label is 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_numbers = range(0, 10, 1)  # Change me, please.\n",
    "\n",
    "def load_prediction_dataset():\n",
    "    dataset = (x_test[image_numbers], y_test[image_numbers])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    dataset = dataset.map(scale).batch(len(image_numbers))\n",
    "    return dataset\n",
    "\n",
    "predicted_results = model.predict(load_prediction_dataset())\n",
    "for index, prediction in enumerate(predicted_results):\n",
    "    predicted_value = np.argmax(prediction)\n",
    "    actual_value = y_test[image_numbers[index]]\n",
    "    if actual_value != predicted_value:\n",
    "        print(\"image number: \" + str(image_numbers[index]))\n",
    "        print(\"the prediction was \" + str(predicted_value))\n",
    "        print(\"the actual label is \" + str(actual_value))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_image_number = 8\n",
    "plt.imshow(x_test[bad_image_number].reshape(HEIGHT, WIDTH));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's understandable why the poor computer would have some trouble. Some of these images are difficult for even humans to read. In fact, we can see what the computer thinks each digit looks like.\n",
    "\n",
    "Each of the 10 neurons in the dense layer of our model has 785 weights feeding into it. That's 1 weight for every pixel in the image + 1 for a bias term. These weights are flattened feeding into the model, but we can reshape them back into the original image dimensions to see what the computer sees.\n",
    "\n",
    "**TODO 3**: Reshape the layer weights to be the shape of an input image and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0f00673ed0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DIGIT = 4  # Change me to be an integer from 0 to 9.\n",
    "LAYER = 1  # Layer 0 flattens image, so no weights\n",
    "WEIGHT_TYPE = 0  # 0 for variable weights, 1 for biases\n",
    "\n",
    "dense_layer_weights = model.layers[LAYER].get_weights()\n",
    "digit_weights = dense_layer_weights[WEIGHT_TYPE][:, DIGIT]\n",
    "plt.imshow(digit_weights.reshape((HEIGHT, WIDTH)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you recognize the digit the computer was trying to learn? Pretty trippy, isn't it! Even with a simple \"brain\", the computer can form an idea of what a digit should be. The human brain, however, uses [layers and layers of calculations for image recognition](https://www.salk.edu/news-release/brain-recognizes-eye-sees/). Ready for the next challenge? <a href=\"https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/images/mnist_linear.ipynb\">Click here</a> to super charge our models with human-like vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise\n",
    "\n",
    "Want to push your understanding further? Instead of using Keras' built in layers, try repeating the above exercise with your own [custom layers](https://www.tensorflow.org/tutorials/eager/custom_layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
